(base) siddharthlala@Siddharths-MacBook-Pro ~ % cd /Users/siddharthlala/Desktop/Experiments/E2
(base) siddharthlala@Siddharths-MacBook-Pro E2 % source ~/cifar10_env/bin/activate
python FL_model_testing.py
Loading test data...
100.0%
Loading trained global model...
Client 0: 1487 samples
Client 1: 4932 samples
Client 2: 5922 samples
Client 3: 4088 samples
Client 4: 4105 samples
Client 5: 4759 samples
Client 6: 6296 samples
Client 7: 7027 samples
Client 8: 5150 samples
Client 9: 6234 samples
2025-03-19 04:59:02.978 Python[81681:15201752] +[IMKClient subclass]: chose IMKClient_Modern
2025-03-19 04:59:02.978 Python[81681:15201752] +[IMKInputSession subclass]: chose IMKInputSession_Modern

=== FL Round 1 ===
Training Client 8...
Epoch 1, Loss: 1.7612090636484372
Epoch 2, Loss: 1.4837749048789837
Epoch 3, Loss: 1.3560124187736038
Training Client 1...
Epoch 1, Loss: 2.0708274256798527
Epoch 2, Loss: 1.8787906692874048
Epoch 3, Loss: 1.7472842631801482
Training Client 6...
Epoch 1, Loss: 2.0557830152172727
Epoch 2, Loss: 1.8567685693653706
Epoch 3, Loss: 1.6497514937734845
Training Client 7...
Epoch 1, Loss: 1.9269302080978046
Epoch 2, Loss: 1.754241734201258
Epoch 3, Loss: 1.6445159841667523
Training Client 3...
Epoch 1, Loss: 1.6846870556473732
Epoch 2, Loss: 1.2492908756248653
Epoch 3, Loss: 1.1105658998712897
Global model saved as global_model.pth

=== FL Round 2 ===
Training Client 4...
Epoch 1, Loss: 1.8860247477080472
Epoch 2, Loss: 1.5732552598613176
Epoch 3, Loss: 1.4329143023306086
Training Client 6...
Epoch 1, Loss: 2.022032219141268
Epoch 2, Loss: 1.8128615956621121
Epoch 3, Loss: 1.641287627559023
Training Client 5...
Epoch 1, Loss: 2.2089104100361765
Epoch 2, Loss: 2.085573199611382
Epoch 3, Loss: 1.9222240063968121
Training Client 1...
Epoch 1, Loss: 2.061341199567241
Epoch 2, Loss: 1.8679161125613797
Epoch 3, Loss: 1.7269760700964158
Training Client 9...
Epoch 1, Loss: 2.0137133818406325
Epoch 2, Loss: 1.769555212289859
Epoch 3, Loss: 1.6120900618724334
Global model saved as global_model.pth

=== FL Round 3 ===
Training Client 5...
Epoch 1, Loss: 2.2166333758591006
Epoch 2, Loss: 2.0738300789122612
Epoch 3, Loss: 1.9339593840925486
Training Client 9...
Epoch 1, Loss: 1.9772531845630743
Epoch 2, Loss: 1.739811039582277
Epoch 3, Loss: 1.5900767479187403
Training Client 7...
Epoch 1, Loss: 1.9484242704781618
Epoch 2, Loss: 1.7355505580251867
Epoch 3, Loss: 1.6252580810676922
Training Client 3...
Epoch 1, Loss: 1.7258002534508705
Epoch 2, Loss: 1.2688263640739024
Epoch 3, Loss: 1.1187578882090747
Training Client 1...
Epoch 1, Loss: 2.064460172960835
Epoch 2, Loss: 1.857484842115833
Epoch 3, Loss: 1.7553918553936867
Global model saved as global_model.pth

=== FL Round 4 ===
Training Client 5...
Epoch 1, Loss: 2.2290029285738133
Epoch 2, Loss: 2.095310373594297
Epoch 3, Loss: 1.9648573454594451
Training Client 7...
Epoch 1, Loss: 1.8901505714113063
Epoch 2, Loss: 1.710735028440302
Epoch 3, Loss: 1.5897534608840942
Training Client 2...
Epoch 1, Loss: 1.7698554531220467
Epoch 2, Loss: 1.4766208949268504
Epoch 3, Loss: 1.3558312990973074
Training Client 9...
Epoch 1, Loss: 2.015351227002266
Epoch 2, Loss: 1.7516742443427062
Epoch 3, Loss: 1.5953986467459262
Training Client 8...
Epoch 1, Loss: 1.8222974509185885
Epoch 2, Loss: 1.532388832998572
Epoch 3, Loss: 1.401969569810429
Global model saved as global_model.pth

=== FL Round 5 ===
Training Client 3...
Epoch 1, Loss: 1.6499793585389853
Epoch 2, Loss: 1.2627320224419236
Epoch 3, Loss: 1.096790811046958
Training Client 4...
Epoch 1, Loss: 1.9292787877164146
Epoch 2, Loss: 1.617679098779841
Epoch 3, Loss: 1.4487616932669352
Training Client 7...
Epoch 1, Loss: 1.910928808559071
Epoch 2, Loss: 1.718506247888912
Epoch 3, Loss: 1.6182572733272205
Training Client 1...
Epoch 1, Loss: 2.059770057278295
Epoch 2, Loss: 1.8714994761251633
Epoch 3, Loss: 1.765590432382399
Training Client 2...
Epoch 1, Loss: 1.7915878084398085
Epoch 2, Loss: 1.4864701913890017
Epoch 3, Loss: 1.3683180696861719
Global model saved as global_model.pth

=== FL Round 6 ===
Training Client 5...
Epoch 1, Loss: 2.223939914831379
Epoch 2, Loss: 2.071760451233627
Epoch 3, Loss: 1.9556378646184933
Training Client 4...
Epoch 1, Loss: 1.8843657813330954
Epoch 2, Loss: 1.5961560454479484
Epoch 3, Loss: 1.4541461398435194
Training Client 9...
Epoch 1, Loss: 2.034863983667814
Epoch 2, Loss: 1.7833636669012216
Epoch 3, Loss: 1.6364878037036994
Training Client 6...
Epoch 1, Loss: 2.041353036909539
Epoch 2, Loss: 1.8559183560047052
Epoch 3, Loss: 1.690786748368123
Training Client 1...
Epoch 1, Loss: 2.057133403901131
Epoch 2, Loss: 1.8722741350050895
Epoch 3, Loss: 1.7373127391261438
Global model saved as global_model.pth

=== FL Round 7 ===
Training Client 3...
Epoch 1, Loss: 1.6628013923764229
Epoch 2, Loss: 1.2658011768944561
Epoch 3, Loss: 1.1167029398493469
Training Client 8...
Epoch 1, Loss: 1.752095928103287
Epoch 2, Loss: 1.481169581413269
Epoch 3, Loss: 1.3615953459502748
Training Client 1...
Epoch 1, Loss: 2.075138021284534
Epoch 2, Loss: 1.886770744477549
Epoch 3, Loss: 1.7743366026109264
Training Client 0...
Epoch 1, Loss: 2.256168816951995
Epoch 2, Loss: 2.1249684957747763
Epoch 3, Loss: 2.033691464586461
Training Client 2...
Epoch 1, Loss: 1.7816871532829859
Epoch 2, Loss: 1.4658214283245865
Epoch 3, Loss: 1.3588806579830826
Global model saved as global_model.pth

Federated Learning Training Complete!
Evaluating model on test set...
Generating confusion matrix...
Model evaluation completed!
Test Accuracy: 0.1138, Test Loss: 2.3013
(cifar10_env) (base) siddharthlala@Siddharths-MacBook-Pro E2 % 
